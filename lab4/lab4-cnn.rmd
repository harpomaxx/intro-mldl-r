---
title: "Introduction To Machine Learning and Deep Learning with R. LAB 4: The MNIST dataset CNN"
output: html_notebook
---

Load the Mnist Dataset, available from keras package.

```{r, results='hide'}
library(keras)

mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
```

The images are encoded as as 3D arrays, and the labels are a 1D array of digits, ranging from 0 to 9. There is a one-to-one correspondence between the images and the labels.
```{r}
str(train_images)
```


Before training, we'll preprocess the data by reshaping it into the shape the CNN network expects. Usually A CNN takes as input tensors of shape (image_height, image_width, image_channels) (not including the batch dimension). In this case, since images are in grayscale, only 1 channel is needed.

```{r}
#train_images <- array_reshape(train_images, c(60000, 28 * 28))


train_images <- array_reshape(train_images, c(nrow(train_images), 28, 28, 1))
test_images <- array_reshape(test_images, c(nrow(test_images), 28, 28, 1))
input_shape <- c(28, 28, 1)
train_images <- train_images / 255
test_images <- test_images / 255


str(train_images_a)
```


Let's build the network.

```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
                input_shape = c(28,28,1)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% 
  #layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  #layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% 
  layer_flatten() %>% 
  layer_dense(units = 128, activation = 'relu') %>% 
  layer_dense(units = 10, activation = 'softmax')

model %>% summary()
```



As you can see, the (3, 3, 64) outputs are  into vectors of shape (576) before going through two dense layers.



```{r}
# Compile model
model %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```



We also need to categorically encode the labels:

```{r}
train_labels_c <- to_categorical(train_labels)
test_labels_c <- to_categorical(test_labels)

head(test_labels_c)
```

We are now ready to train our network, 

```{r, echo=TRUE, results='hide'}
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 80 == 0) cat("\n")
    cat(".")
  }
)  
model %>% fit(train_images, 
              train_labels_c,
              epochs = 10, 
              batch_size = 128, 
              validation_split=0.2,
              verbose=0,
              callbacks = list(print_dot_callback)
  
              )
```



```{r}

test_results<-model %>% predict_classes(test_images, batch_size=128)
caret::confusionMatrix(as.factor(test_results) ,as.factor(test_labels))
```

