---
title: "Introduction To Machine Learning and Deep Learning with R. LAB 3: The Boston dataset"
output: html_notebook
---

```{r}
boston_housing <- dataset_boston_housing()

c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
```

The dataset contains 13 different features:

* Per capita crime rate.
* The proportion of residential land zoned for lots over 25,000 square feet.
* The proportion of non-retail business acres per town.
* Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
* Nitric oxides concentration (parts per 10 million).
* The average number of rooms per dwelling.
* The proportion of owner-occupied units built before 1940.
* Weighted distances to five Boston employment centers.
* Index of accessibility to radial highways.
* Full-value property-tax rate per $10,000.
* Pupil-teacher ratio by town.
* 1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.
* Percentage lower status of the population.

```{r}
library(tibble)

column_names <- c('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 
                  'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT')
train_df <- as_tibble(train_data)
colnames(train_df) <- column_names

train_df
```
##Labels
The labels are the house prices in thousands of dollars. (You may notice the mid-1970s prices.)

```{r}
train_labels[1:10] # Display first 10 entries
```
##Normalize features
```{r}
# Test data is *not* used when calculating the mean and std.

# Normalize training data
train_data <- scale(train_data) 

# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center") 
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)

train_data[1, ] # First training sample, normalized
```

```{r}
build_model <- function() {
  
  model <- keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu",
                input_shape = dim(train_data)[2]) %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 1) # when doing regresion, no activation function is used.
  
  model %>% compile(
    loss = "mse",
    optimizer = "rmsprop",
    metrics = list("mean_absolute_error")
  )
  
  model
}

model <- build_model()
model %>% summary()
```
```{r}
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 80 == 0) cat("\n")
    cat(".")
  }
)    


# Fit the model and store training stats
history <- model %>% fit(
  train_data,
  train_labels,
  epochs = 500,
  batch_size = 128,
  validation_split = 0.2,
  callbacks = list(print_dot_callback),
  
  verbose = 0)


plot(history)
```
```{r}
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)

model <- build_model()
history <- model %>% fit(
  train_data,
  train_labels,
  epochs = 500,
  validation_split = 0.2,
  verbose = 0,
  callbacks = list(early_stop)
)
```


```{r}
c(loss, mae) %<-% (model %>% evaluate(test_data, test_labels, verbose = 0))
paste0("Mean absolute error on test set: $", sprintf("%.2f", mae * 1000))
```
#Using CARET
```{r}
test_data_resuls <- model %>% predict(test_data)
caret::postResample(test_data_resuls,test_labels)
```

