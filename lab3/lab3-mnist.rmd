---
title: "Introduction To Machine Learning and Deep Learning with R. LAB 3: The MNIST dataset"
output: html_notebook
---

Load the Mnist Dataset, available from keras package.

```{r, results='hide'}
library(keras)

mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
```

The images are encoded as as 3D arrays, and the labels are a 1D array of digits, ranging from 0 to 9. There is a one-to-one correspondence between the images and the labels.

The R `str()` function is a convenient way to get a quick glimpse at the structure of an array. Let's use it to have a look at the training data:

```{r}
str(train_images)
```

```{r}
str(train_labels)
```

Let's have a look at the test data:

```{r}
str(test_images)
```

```{r}
str(test_labels)
```

Let's plot it...
```{r}
digit <- train_images[5,,]
plot(as.raster(digit, max = 255))
```


Let's build the network

```{r}
network <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
  layer_dense(units = 10, activation = "softmax")
```


Here our network consists of a sequence of two layers, which are densely connected (also called _fully connected_) neural layers. The second (and last) layer is a 10-way _softmax_ layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.


```{r}
network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

network %>% summary()
```

Before training, we'll preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the `[0, 1]` interval. Previously, our training images, for instance, were stored in an array of shape `(60000, 28, 28)` of type integer with values in the `[0, 255]` interval. We transform it into a double array of shape `(60000, 28 * 28)` with values between 0 and 1.

```{r}
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255

test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
head(test_images)
```

We also need to categorically encode the labels:

```{r}
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)

head(test_labels)
```

We are now ready to train our network, 

```{r, echo=TRUE, results='hide'}
network %>% fit(train_images, train_labels, epochs = 10, batch_size = 128, validation_split=0.2 )
```

Two quantities are being displayed during training: the "loss" of the network over the training data, and the accuracy of the network over the training data.

We quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now let's check that our model performs well on the test set too:

```{r}

test_results<-network %>% predict_classes(test_images, batch_size=4096)
caret::confusionMatrix(as.factor(test_results) ,as.factor(test_labels))
```




